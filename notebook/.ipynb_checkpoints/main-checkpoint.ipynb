{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Incident Reporter with Python\n",
    "\n",
    "This notebook contains the code necessary to create the SFPD Incident and Crime Rate Reporter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:36:12.710034Z",
     "start_time": "2021-01-21T06:36:12.534948Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import json\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# The config file is hidden since it contains the api key \n",
    "# needed to get data from the sfpd database\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Client and Querying the Database\n",
    "\n",
    "Via `data.sfgov.org`, we can access many databases including the SFPD database which records all occuring incidents in the different police districts. For us to query it, we need to instantiate our client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:36:49.178645Z",
     "start_time": "2021-01-21T06:36:49.175644Z"
    }
   },
   "outputs": [],
   "source": [
    "client = Socrata(\"data.sfgov.org\",\n",
    "                 config.api_key,\n",
    "                 username=config.username,\n",
    "                 password=config.password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know get the latest 15,000 reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.get(\"wg3w-h783\", limit=15000, order=\"incident_date DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wg3w-h783` is the database `ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we convert the response to a pandas dataframe followed by formating the datetime\n",
    "results_df = pd.DataFrame.from_records(results)\n",
    "results_df['incident_datetime'] = pd.to_datetime(results_df['incident_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Specific Information \n",
    "\n",
    "Having loaded the 15000 incidents into our dataframe, we can group the data according to police districts. This will allow us to create a crime rate graph for each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_mining/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda3/envs/data_mining/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Since we only care about crime rate we just need the time and the district of the event\n",
    "district_df = results_df[[\"incident_datetime\", \"police_district\"]]\n",
    "# Each row represents an event\n",
    "district_df[\"Count\"] = 1\n",
    "# Formatting the datetime\n",
    "district_df[\"incident_datetime\"] = district_df.incident_datetime.dt.date\n",
    "# Grouping the data by date and district\n",
    "district_df = district_df.groupby(['incident_datetime','police_district'], as_index=False)['Count'].sum()\n",
    "district_df.incident_datetime = district_df.incident_datetime.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Crime Rate\n",
    "\n",
    "Similarly, we can group the main database by date to get a sense of the total crime rate per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "total_df = results_df[[\"incident_datetime\"]]\n",
    "total_df[\"Count\"] = 1\n",
    "# This groups the dataset by day\n",
    "total_df = total_df.groupby(total_df.incident_datetime.dt.date)[['Count']].sum()\n",
    "total_df = total_df.iloc[1:]\n",
    "total_df = total_df.reset_index()\n",
    "total_df.incident_datetime = total_df.incident_datetime.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Rolling Average\n",
    "\n",
    "Having our total crime rate, it is now possible to get the rolling mean. In this case we select a five-day window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = total_df.rolling(5).mean().fillna(method=\"backfill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding two more days for ML inference\n",
    "\n",
    "In order to predict the future, rows containing future dates must be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract the last day avaliable in the dataset\n",
    "d = datetime.strptime(total_df.incident_datetime.values[-1].replace(\"-\", \"/\"), \"%Y/%m/%d\")\n",
    "# Then, we create 2 more rows belonging to two more days after the last date avaliable\n",
    "rng = pd.date_range(d, periods=3, freq='d')\n",
    "rng = pd.to_datetime(rng, format='%Y%m%d')\n",
    "\n",
    "# We transform the new dates into a dataframe\n",
    "to_append = pd.DataFrame({'incident_datetime': rng}) \n",
    "to_append[\"incident_datetime\"] = to_append.incident_datetime.dt.date\n",
    "to_append = to_append.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we fill the new values with null so that ChartJS does not plot them\n",
    "total_df = total_df.append(to_append).fillna(\"null\")\n",
    "total_df.incident_datetime = total_df.incident_datetime.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we need to add null values to the rolling average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = list(mean.Count.values)\n",
    "mean.extend(to_append.shape[0] * [\"null\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the `template.html` file to create the new `index.html`\n",
    "\n",
    "There are various ways to achieve this. There is definitely a better way using JavaScript but for the purposees of this project we will stick with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the template html file\n",
    "with open('../template.html', 'r') as file :\n",
    "    filedata = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the dates\n",
    "filedata = filedata.replace('          labels: [], // INCLUDE X AXIS DATES', \n",
    "                            '          labels: {}, // INCLUDE X AXIS DATES'.format(\n",
    "                                list(total_df.incident_datetime.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the total crime rate information\n",
    "filedata = filedata.replace('            data: [], // Total', \n",
    "                            '            data: {}, // Total'.format(\n",
    "                                list(total_df.Count.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the rolling average values\n",
    "filedata = filedata.replace('            data: [], // Average', \n",
    "                            '            data: {}, // Average'.format(\n",
    "                                mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting district-depedent crime rate information\n",
    "for i in district_df.police_district.unique():\n",
    "    if i != \"Out of SF\":\n",
    "        district_info = district_df[district_df.police_district == i]\n",
    "        filedata = filedata.replace('            data: [], // {}'.format(i), \n",
    "                                    '            data: {}, // {}'.format(list(district_info.Count.values), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file out again\n",
    "with open('../index.html', 'w') as file:\n",
    "    file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the Five Latest Reports and Creating a Boostrap Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_reports = results_df.copy()\n",
    "\n",
    "latest_reports = latest_reports[[\"incident_datetime\", \"police_district\", \"incident_category\", \n",
    "                                 \"incident_subcategory\", \"incident_description\", \"resolution\", \n",
    "                                 \"intersection\"]]\n",
    "\n",
    "latest_reports.columns = [\"Incident Date\", \"District\", \"Incident Category\", \"Subcategory\", \n",
    "                          \"Description\", \"Resolution\", \"Intersection\"]\n",
    "\n",
    "# We save the dataframe as html with the table and table-hover class\n",
    "latest_reports.head(5).to_html(open('../table.html', 'w'), classes=[\"table\", \"table-hover\"], \n",
    "                               border=0, index=False, justify=\"center\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now insert the html table into the updated index html table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../index.html\", \"r\") as f1:\n",
    "    t1 = f1.readlines()\n",
    "with open(\"../table.html\", \"r\") as f2:\n",
    "    t2 = f2.readlines()\n",
    "\n",
    "initial = 88\n",
    "for i in range(0,len(t2)):\n",
    "    t1.insert(initial, t2[i])\n",
    "    initial = initial + 1\n",
    "\n",
    "with open(\"../index.html\", \"w\") as f2:\n",
    "    f2.writelines(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For asthetics, we provide the `thead-dark` class to our table head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../index.html','r') as file:\n",
    "    filedata = file.read()\n",
    "    filedata = filedata.replace('  <thead>','  <thead class=\"thead-dark\">')\n",
    "with open('../index.html','w') as file:\n",
    "    file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chloropleth Map \n",
    "\n",
    "To create the chloropleth map showing the the latest weekly crime rate we need to resample the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_mining/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# RESAMPLING THE DATAFRAME\n",
    "weekly_df = results_df[[\"incident_datetime\", \"police_district\"]]\n",
    "weekly_df[\"Count\"] = 1\n",
    "weekly_df = weekly_df.set_index(weekly_df[\"incident_datetime\"]).drop(columns=[\"incident_datetime\"])\n",
    "weekly_df = weekly_df.groupby('police_district').resample('1w').sum()\n",
    "weekly_df = weekly_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having grouped the dataset into a weekly manner, we can extract the previous to last row. The dataset is updated daily but the incidents are reported after these are approved which is not always in a timely manner. this is why the previous to last entrie is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week = pd.DataFrame(columns=weekly_df.columns)\n",
    "\n",
    "for i in weekly_df.police_district.unique():\n",
    "    last_week = last_week.append(weekly_df[weekly_df.police_district == i].iloc[[-2]])\n",
    "\n",
    "last_week = last_week[[\"police_district\", \"Count\"]]\n",
    "last_week.columns = [\"Neighborhood\", \"Count\"]\n",
    "last_week.Neighborhood = last_week.Neighborhood.apply(lambda x: x.upper())\n",
    "last_week.Count = last_week.Count.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Processing GeoJson File\n",
    "\n",
    "The GeoJSON file contains the coordinates needed to plot the different districts in the san francisco area. This file is needed by plotly to create a map plot using various plotters including folium and plotly. In this notebook we use the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Francisco latitude and longitude values\n",
    "latitude = 37.77\n",
    "longitude = -122.42\n",
    "\n",
    "# loading up json file\n",
    "with open(\"../src/sf.geojson\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# creating and id key to hold district info needed by plotly\n",
    "for i in range(0,len(data[\"features\"])):\n",
    "    copy_dict = data[\"features\"][i]\n",
    "    copy_dict[\"id\"] = copy_dict[\"properties\"][\"DISTRICT\"]\n",
    "    data[\"features\"][i] = copy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Plotly Chloropleth Map\n",
    "\n",
    "The plotly map is saved as an html file and inserted into our updated `index.html` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(last_week, geojson=data, locations='Neighborhood', color='Count',\n",
    "                           color_continuous_scale=\"Turbo\",\n",
    "                           range_color=(0, last_week.Count.max()),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=11, center = {\"lat\": latitude, \"lon\": longitude},\n",
    "                           opacity=0.5,\n",
    "                           labels={\"Count\":\"Weekly Crime Rate\"}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"../plotly_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Incident Category Distribution Plotly Bar Plot\n",
    "\n",
    "We can also create a bar plot to visualize the distribution of the latest 15000 incident categories. For this we group the data by category rather than by date or district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Count\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_category = results_df.groupby(\"incident_category\")[[\"Count\"]].sum().reset_index().sort_values(by=\"Count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(incident_category, x='incident_category', y='Count', \n",
    "             labels={\"incident_category\":\"Incident Category\"}, color=\"Count\", opacity=1.0)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)', 'paper_bgcolor': 'rgba(0, 0, 0, 0)'})\n",
    "fig.write_html(\"../incident_category.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this section we create a simple Decision Tree model to predict the crime rate in the next pair of dates. At the begging we extracted only the latest 15,000 incidents. To train our model we need more data. For this we query the database for 500,000 rows which is more than the avaliable datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:37:55.243245Z",
     "start_time": "2021-01-21T06:37:20.198373Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell was ran once and the data was saved into a csv\n",
    "# results_ml = client.get(\"wg3w-h783\", limit=500000, order=\"incident_date DESC\")\n",
    "# df = pd.DataFrame.from_records(results_ml)\n",
    "# df.to_csv(\"sfpd_reports.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:36:38.701337Z",
     "start_time": "2021-01-21T06:36:38.663336Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sfpd_reports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make remove any rows with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:02.647510Z",
     "start_time": "2021-01-21T06:37:59.859875Z"
    }
   },
   "outputs": [],
   "source": [
    "df['incident_datetime'] = pd.to_datetime(df['incident_datetime'])\n",
    "df = df[~df['incident_category'].isnull()]\n",
    "df = df[~df['latitude'].isnull()]\n",
    "df = df[~df['police_district'].isnull()]\n",
    "df = df[[\"incident_datetime\", \"incident_day_of_week\", \"police_district\", \"incident_category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the daily crime rate so we group the response by date. In other words, we resample the dataframe into one day bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:04.004901Z",
     "start_time": "2021-01-21T06:38:03.287096Z"
    }
   },
   "outputs": [],
   "source": [
    "group_by_pd = df.copy()\n",
    "group_by_pd[\"Count\"] = 1\n",
    "group_by_pd = group_by_pd.set_index(group_by_pd[\"incident_datetime\"]).drop(columns=[\"incident_datetime\"])\n",
    "group_by_pd = group_by_pd.groupby(['police_district']).resample('1D').sum()\n",
    "group_by_pd = group_by_pd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:04.417991Z",
     "start_time": "2021-01-21T06:38:04.414991Z"
    }
   },
   "outputs": [],
   "source": [
    "X = group_by_pd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Here, we one-hot encode the police district feature and create a feature for both the day and month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:05.734299Z",
     "start_time": "2021-01-21T06:38:05.727298Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, pd.get_dummies(X[\"police_district\"])], axis=1).drop(columns=[\"police_district\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:06.336793Z",
     "start_time": "2021-01-21T06:38:06.310788Z"
    }
   },
   "outputs": [],
   "source": [
    "# X['year'] = pd.to_datetime(X['incident_datetime']).dt.year\n",
    "X['month'] = pd.to_datetime(X['incident_datetime']).dt.month\n",
    "X['day'] = pd.to_datetime(X['incident_datetime']).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the latest incidents will be approved later in the future we cannot use the last days as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:07.059485Z",
     "start_time": "2021-01-21T06:38:07.055493Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X.iloc[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:07.827915Z",
     "start_time": "2021-01-21T06:38:07.822913Z"
    }
   },
   "outputs": [],
   "source": [
    "y = X[[\"Count\"]]\n",
    "X = X.drop(['incident_datetime', \"Count\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DT Regressor\n",
    "\n",
    "Here we use the scikit-learn implementation to train a DT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T06:38:10.716636Z",
     "start_time": "2021-01-21T06:38:09.878209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr_1 = DecisionTreeRegressor()\n",
    "regr_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Data Inference Pipeline\n",
    "\n",
    "We will get the last eight days for inference plus the future two days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_districts = ['Bayview', 'Central', 'Ingleside', 'Mission', 'Northern', \n",
    "                'Out of SF', 'Park', 'Richmond', 'Southern', 'Taraval', \n",
    "                'Tenderloin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datetime.today() - timedelta(days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range(d, periods=10+2, freq='d')\n",
    "rng = pd.to_datetime(rng, format='%Y%m%d')\n",
    "\n",
    "dates_to_query = pd.DataFrame({ 'Date': rng}) \n",
    "dates_to_query[\"year\"] = dates_to_query.Date.dt.year\n",
    "dates_to_query[\"day\"] = dates_to_query.Date.dt.day\n",
    "dates_to_query[\"month\"] = dates_to_query.Date.dt.month\n",
    "dates_to_query[\"Date\"] = dates_to_query.Date.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dates = dates_to_query.Date.astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_ml = total_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_ml[\"year\"] = pd.to_datetime(total_df_ml.incident_datetime).dt.year\n",
    "total_df_ml[\"day\"] = pd.to_datetime(total_df_ml.incident_datetime).dt.day\n",
    "total_df_ml[\"month\"] = pd.to_datetime(total_df_ml.incident_datetime).dt.month\n",
    "\n",
    "total_df_ml = total_df_ml.drop(columns=[\"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows us to get the total crime rate for each queried data. Since our model is built to predict crime on each neighborhood, we need to sum all predictions to get the total crime rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_for_dates_df(model, dates):\n",
    "    # Iterate through the police districts and sum all predictions\n",
    "    for i in pd_districts:\n",
    "        testing = pd.DataFrame(columns=X.columns)\n",
    "        data = pd.DataFrame({i: 1, \n",
    "#                              \"year\":dates.year.values, \n",
    "                             \"month\":dates.month.values, \n",
    "                             \"day\":dates.day.values})\n",
    "        testing = testing.append(data).fillna(0)\n",
    "        y = model.predict(testing)\n",
    "        dates[i] = y.astype(int)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering and Formatting the Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_mining/lib/python3.7/site-packages/pandas/core/generic.py:5489: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_to_query = get_prediction_for_dates_df(regr_1, dates_to_query)\n",
    "dates_to_query[\"total_crime_rate\"] = dates_to_query.iloc[:, 4:].sum(axis=1)\n",
    "dates_to_query = dates_to_query[[\"Date\", \"total_crime_rate\"]]\n",
    "dates_to_query.columns = [\"incident_datetime\", \"tcr\"]\n",
    "dates_to_query.incident_datetime = dates_to_query.incident_datetime.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_ml = get_prediction_for_dates_df(regr_1, total_df_ml)\n",
    "\n",
    "total_df_ml[\"total_crime_rate\"] = total_df_ml.iloc[:, 4:].sum(axis=1)\n",
    "\n",
    "total_df_ml = total_df_ml[[\"incident_datetime\", \"total_crime_rate\"]]\n",
    "total_df_ml.columns = [\"incident_datetime\", \"tcr\"]\n",
    "total_df_ml.incident_datetime = total_df_ml.incident_datetime.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_predictions = total_df[[\"incident_datetime\"]].iloc[:-2]\n",
    "ai_predictions = pd.merge(ai_predictions, dates_to_query, on=\"incident_datetime\", how=\"outer\").fillna(\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_predictions.loc[ai_predictions.index[50-11], 'tcr'] = total_df.loc[total_df.index[50-11], 'Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out the Predictions to our `index.html` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "with open('../index.html', 'r') as file :\n",
    "    filedata = file.read()\n",
    "    \n",
    "filedata = filedata.replace('            data: [], // AI', \n",
    "                            '            data: {}, // AI'.format(\n",
    "                                list(ai_predictions.tcr.values)))\n",
    "\n",
    "# Write the file out again\n",
    "with open('../index.html', 'w') as file:\n",
    "    file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
